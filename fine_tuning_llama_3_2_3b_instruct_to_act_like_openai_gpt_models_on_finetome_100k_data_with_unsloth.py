# -*- coding: utf-8 -*-
"""Fine-tuning-Llama-3.2-3B-Instruct-To-Act-like-OpenAI-GPT-Models-On-FineTome-100k-Data-With_Unsloth

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ErbG_-fuQKQSgr2BrTdWFqbqj5uMOZ_m

## Behave Llama as GPT
"""

# gpu must be enabled else unsloth give error
pip install unsloth transformers trl

import torch
from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth.chat_templates import get_chat_template, standardize_sharegpt

#this will downlaod 2.35 gb of model and other config jason, unsloth fast downloding enabled

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct",
    max_seq_length= 2048,   #max sequence a model can process, context window size for this model, amount of token we can passing in the single turn of msg ex. in converstion, all within 2048 token
    load_in_4bit =True)    # reduce the memory with 4 bit, as it is 3 billion parameter model

#parameter efficient fine tuning model
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # r=16 is rank parameter for Lora
    target_modules = ["q_proj", "k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"] # query  projection, key projection, value proj, output, gate, up, down...important
)

tokenier = get_chat_template(tokenizer, chat_template= "llama-3.1")

# it will download 117 MB of dataset from huggiface
#url of dataset: https://huggingface.co/datasets/mlabonne/FineTome-100k

dataset = load_dataset("mlabonne/FineTome-100k", split ="train")

dataset = standardize_sharegpt(dataset)

dataset

dataset[0]

dataset = dataset.map(
    lambda examples: {
        "text":[
            tokenier.apply_chat_template(convo, tokenize= False)
            for convo in examples["conversations"]
        ]
    },
    batched = True
)

dataset[0]

trainer = SFTTrainer(
    model = model,
    train_dataset = dataset,
    dataset_text_field ="text",
    max_seq_length = 2048,
    args= TrainingArguments(
        per_device_train_batch_size =2, #faster
        gradient_accumulation_steps = 4,
        warmup_steps=5,
        max_steps =60,
        learning_rate = 2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps =1,
        output_dir ="outputs"
    ),
)

# this will ask
trainer.train()

model.save_pretrained("finetuned_model")

inference_model,inference_tokenizer = FastLanguageModel.from_pretrained(
    model_name = "./finetuned_model",
    max_seq_length = 2048,
    load_in_4bit = True
)

text_prompts = [
    "what are the key principles of investment?"
]

for prompt in text_prompts:
  formatted_prompt = inference_tokenizer.apply_chat_template([{
      "role":"user",
      "content":prompt
  }],tokenize = False)

  model_inputs = inference_tokenizer(formatted_prompt,return_tensors="pt").to("cuda")
  generated_ids = inference_model.generate(
      **model_inputs,
      max_new_tokens= 512,
      temperature =0.7,
      do_sample=True,
      pad_token_id = inference_tokenizer.pad_token_id
  )
  response = inference_tokenizer.batch_decode(generated_ids, skip_special_tokens= True)[0]
  print(response)

"""## Thanks"""